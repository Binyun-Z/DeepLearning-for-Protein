{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tape:\n",
      "\n",
      "NAME\n",
      "    tape\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    datasets\n",
      "    errors\n",
      "    main\n",
      "    metrics\n",
      "    models (package)\n",
      "    optimization\n",
      "    registry\n",
      "    tokenizers\n",
      "    training\n",
      "    utils (package)\n",
      "    visualization\n",
      "\n",
      "SUBMODULES\n",
      "    imported_module\n",
      "\n",
      "DATA\n",
      "    name = 'PairwiseContactPredictionHead'\n",
      "\n",
      "VERSION\n",
      "    0.5\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tape\n",
    "help(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module tape.tokenizers in tape:\n",
      "\n",
      "NAME\n",
      "    tape.tokenizers\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        TAPETokenizer\n",
      "    \n",
      "    class TAPETokenizer(builtins.object)\n",
      "     |  TAPETokenizer(vocab: str = 'iupac')\n",
      "     |  \n",
      "     |  TAPE Tokenizer. Can use different vocabs depending on the model.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab: str = 'iupac')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add_special_tokens(self, token_ids: List[str]) -> List[str]\n",
      "     |      Adds special tokens to the a sequence for sequence classification tasks.\n",
      "     |      A BERT sequence has the following format: [CLS] X [SEP]\n",
      "     |  \n",
      "     |  convert_id_to_token(self, index: int) -> str\n",
      "     |      Converts an index (integer) in a token (string/unicode) using the vocab.\n",
      "     |  \n",
      "     |  convert_ids_to_tokens(self, indices: List[int]) -> List[str]\n",
      "     |  \n",
      "     |  convert_token_to_id(self, token: str) -> int\n",
      "     |      Converts a token (str/unicode) in an id using the vocab.\n",
      "     |  \n",
      "     |  convert_tokens_to_ids(self, tokens: List[str]) -> List[int]\n",
      "     |  \n",
      "     |  convert_tokens_to_string(self, tokens: str) -> str\n",
      "     |      Converts a sequence of tokens (string) in a single string.\n",
      "     |  \n",
      "     |  encode(self, text: str) -> numpy.ndarray\n",
      "     |  \n",
      "     |  tokenize(self, text: str) -> List[str]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_pretrained(**kwargs) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  mask_token\n",
      "     |  \n",
      "     |  start_token\n",
      "     |  \n",
      "     |  stop_token\n",
      "     |  \n",
      "     |  vocab_size\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    IUPAC_CODES = OrderedDict([('Ala', 'A'), ('Asx', 'B'), ('Cys',... 'W')...\n",
      "    IUPAC_VOCAB = OrderedDict([('<pad>', 0), ('<mask>', 1), ('<cls...25), ...\n",
      "    List = typing.List\n",
      "    UNIREP_VOCAB = OrderedDict([('<pad>', 0), ('M', 1), ('R', 2), (...', 2...\n",
      "    logger = <Logger tape.tokenizers (WARNING)>\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/tokenizers.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tape' has no attribute 'visualization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bneumann/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb#ch0000002vscode-remote?line=0'>1</a>\u001b[0m help(tape\u001b[39m.\u001b[39;49mvisualization())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tape' has no attribute 'visualization'"
     ]
    }
   ],
   "source": [
    "help(tape.visualization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tape' has no attribute 'optimization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bneumann/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m help(tape\u001b[39m.\u001b[39;49moptimization)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tape' has no attribute 'optimization'"
     ]
    }
   ],
   "source": [
    "help(tape.optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tape.models in tape:\n",
      "\n",
      "NAME\n",
      "    tape.models\n",
      "\n",
      "DESCRIPTION\n",
      "    # from .modeling_utils import ProteinConfig  # noqa: F401\n",
      "    # from .modeling_utils import ProteinModel  # noqa: F401\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    file_utils\n",
      "    modeling_bert\n",
      "    modeling_lstm\n",
      "    modeling_onehot\n",
      "    modeling_resnet\n",
      "    modeling_trrosetta\n",
      "    modeling_unirep\n",
      "    modeling_utils\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/models/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ProteinBertModel in module tape.models.modeling_bert:\n",
      "\n",
      "class ProteinBertModel(ProteinBertAbstractModel)\n",
      " |  ProteinBertModel(config)\n",
      " |  \n",
      " |  An abstract class to handle weights initialization and\n",
      " |  a simple interface for dowloading and loading pretrained models.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ProteinBertModel\n",
      " |      ProteinBertAbstractModel\n",
      " |      tape.models.modeling_utils.ProteinModel\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, config)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  forward(self, input_ids, input_mask=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      " |  \n",
      " |  base_model_prefix = 'bert'\n",
      " |  \n",
      " |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      " |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      " |      configuration of a `ProteinBertModel`.\n",
      " |      \n",
      " |      \n",
      " |      Arguments:\n",
      " |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      " |              `ProteinBertModel`.\n",
      " |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      " |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      " |          num_attention_heads: Number of attention heads for each attention layer in\n",
      " |              the ProteinBert encoder.\n",
      " |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      " |              layer in the ProteinBert encoder.\n",
      " |          hidden_act: The non-linear activation function (function or string) in the\n",
      " |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      " |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      " |              layers in the embeddings, encoder, and pooler.\n",
      " |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      " |              probabilities.\n",
      " |          max_position_embeddings: The maximum sequence length that this model might\n",
      " |              ever be used with. Typically set this to something large just in case\n",
      " |              (e.g., 512 or 1024 or 2048).\n",
      " |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      " |              `ProteinBertModel`.\n",
      " |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      " |              initializing all weight matrices.\n",
      " |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      " |  \n",
      " |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      " |  \n",
      " |  init_weights(self)\n",
      " |      Initialize and prunes weights if needed.\n",
      " |  \n",
      " |  prune_heads(self, heads_to_prune)\n",
      " |      Prunes heads of the base model.\n",
      " |      \n",
      " |      Arguments:\n",
      " |      \n",
      " |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      " |              associated values being the list of heads to prune in said layer\n",
      " |              (list of `int`).\n",
      " |  \n",
      " |  resize_token_embeddings(self, new_num_tokens=None)\n",
      " |      Resize input token embeddings matrix of the model if\n",
      " |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      " |          afterwards if the model class has a `tie_weights()` method.\n",
      " |      \n",
      " |      Arguments:\n",
      " |      \n",
      " |          new_num_tokens: (`optional`) int:\n",
      " |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      " |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      " |              from the end. If not provided or None: does nothing and just returns a\n",
      " |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      " |      \n",
      " |      Return: ``torch.nn.Embeddings``\n",
      " |          Pointer to the input tokens Embeddings Module of the model\n",
      " |  \n",
      " |  save_pretrained(self, save_directory)\n",
      " |      Save a model and its configuration file to a directory, so that it\n",
      " |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      " |      ` class method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      " |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      " |      \n",
      " |      The model is set in evaluation mode by default using ``model.eval()``\n",
      " |      (Dropout modules are deactivated)\n",
      " |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      " |      \n",
      " |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      " |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      " |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      " |      \n",
      " |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      " |      by YYY, therefore those weights are discarded.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          pretrained_model_name_or_path: either:\n",
      " |      \n",
      " |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      " |                or download, e.g.: ``bert-base-uncased``.\n",
      " |              - a path to a `directory` containing model weights saved using\n",
      " |                :func:`~ProteinModel.save_pretrained`,\n",
      " |                e.g.: ``./my_model_directory/``.\n",
      " |      \n",
      " |          model_args: (`optional`) Sequence of positional arguments:\n",
      " |              All remaning positional arguments will be passed to the underlying model's\n",
      " |              ``__init__`` method\n",
      " |      \n",
      " |          config: (`optional`) instance of a class derived from\n",
      " |              :class:`~ProteinConfig`: Configuration for the model to\n",
      " |              use instead of an automatically loaded configuation. Configuration can be\n",
      " |              automatically loaded when:\n",
      " |      \n",
      " |              - the model is a model provided by the library (loaded with the\n",
      " |                ``shortcut-name`` string of a pretrained model), or\n",
      " |              - the model was saved using\n",
      " |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      " |                by suppling the save directory.\n",
      " |              - the model is loaded by suppling a local directory as\n",
      " |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      " |                `config.json` is found in the directory.\n",
      " |      \n",
      " |          state_dict: (`optional`) dict:\n",
      " |              an optional state dictionnary for the model to use instead of a state\n",
      " |              dictionary loaded from saved weights file. This option can be used if you\n",
      " |              want to create a model from a pretrained configuration but load your own\n",
      " |              weights. In this case though, you should check if using\n",
      " |              :func:`~ProteinModel.save_pretrained` and\n",
      " |              :func:`~ProteinModel.from_pretrained` is not a\n",
      " |              simpler option.\n",
      " |      \n",
      " |          cache_dir: (`optional`) string:\n",
      " |              Path to a directory in which a downloaded pre-trained model\n",
      " |              configuration should be cached if the standard cache should not be used.\n",
      " |      \n",
      " |          force_download: (`optional`) boolean, default False:\n",
      " |              Force to (re-)download the model weights and configuration files and override\n",
      " |              the cached versions if they exists.\n",
      " |      \n",
      " |          resume_download: (`optional`) boolean, default False:\n",
      " |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      " |              such a file exists.\n",
      " |      \n",
      " |          output_loading_info: (`optional`) boolean:\n",
      " |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      " |              unexpected keys and error messages.\n",
      " |      \n",
      " |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      " |              Can be used to update the configuration object (after it being loaded) and\n",
      " |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      " |              depending on whether a `config` is provided or automatically loaded:\n",
      " |      \n",
      " |              - If a configuration is provided with ``config``, ``**kwarg\n",
      " |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      " |                all relevant updates to the configuration have already been done)\n",
      " |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      " |                configuration class initialization function\n",
      " |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      " |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      " |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      " |                that do not correspond to any configuration attribute will be passed to the\n",
      " |                underlying model's ``__init__`` function.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Download model and configuration from S3 and cache.\n",
      " |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      " |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      " |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      " |          # Update configuration during loading\n",
      " |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      " |          assert model.config.output_attention == True\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      " |  \n",
      " |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to float datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor): buffer to be registered.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter): parameter to be added to the module.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.ProteinBertModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tokenize in module tape.tokenizers:\n",
      "\n",
      "tokenize(self, text: str) -> List[str]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.TAPETokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tape.TAPETokenizer('iupac')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize('ABC')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TAPETokenizer' object has no attribute 'convert_token_to_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bneumann/home/bli/Binyun/classify_project/dl_method/test-tape.ipynb#ch0000009vscode-remote?line=0'>1</a>\u001b[0m fe \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mconvert_token_to_ids(tokens)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TAPETokenizer' object has no attribute 'convert_token_to_ids'"
     ]
    }
   ],
   "source": [
    "fe = tokenizer.convert_token_to_id(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tape:\n",
      "\n",
      "NAME\n",
      "    tape\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    datasets\n",
      "    errors\n",
      "    main\n",
      "    metrics\n",
      "    models (package)\n",
      "    optimization\n",
      "    registry\n",
      "    tokenizers\n",
      "    training\n",
      "    utils (package)\n",
      "    visualization\n",
      "\n",
      "SUBMODULES\n",
      "    imported_module\n",
      "\n",
      "DATA\n",
      "    name = 'PairwiseContactPredictionHead'\n",
      "\n",
      "VERSION\n",
      "    0.5\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tape.models in tape:\n",
      "\n",
      "NAME\n",
      "    tape.models\n",
      "\n",
      "DESCRIPTION\n",
      "    # from .modeling_utils import ProteinConfig  # noqa: F401\n",
      "    # from .modeling_utils import ProteinModel  # noqa: F401\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    file_utils\n",
      "    modeling_bert\n",
      "    modeling_lstm\n",
      "    modeling_onehot\n",
      "    modeling_resnet\n",
      "    modeling_trrosetta\n",
      "    modeling_unirep\n",
      "    modeling_utils\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/models/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module tape.models.modeling_bert in tape.models:\n",
      "\n",
      "NAME\n",
      "    tape.models.modeling_bert - PyTorch BERT model.\n",
      "\n",
      "CLASSES\n",
      "    tape.models.modeling_utils.ProteinConfig(builtins.object)\n",
      "        ProteinBertConfig\n",
      "    tape.models.modeling_utils.ProteinModel(torch.nn.modules.module.Module)\n",
      "        ProteinBertAbstractModel\n",
      "            ProteinBertForContactPrediction\n",
      "            ProteinBertForMaskedLM\n",
      "            ProteinBertForSequenceClassification\n",
      "            ProteinBertForSequenceToSequenceClassification\n",
      "            ProteinBertForValuePrediction\n",
      "            ProteinBertModel\n",
      "    torch.nn.modules.module.Module(builtins.object)\n",
      "        ProteinBertAttention\n",
      "        ProteinBertEmbeddings\n",
      "        ProteinBertEncoder\n",
      "        ProteinBertIntermediate\n",
      "        ProteinBertLayer\n",
      "        ProteinBertOutput\n",
      "        ProteinBertPooler\n",
      "        ProteinBertSelfAttention\n",
      "        ProteinBertSelfOutput\n",
      "    \n",
      "    class ProteinBertAbstractModel(tape.models.modeling_utils.ProteinModel)\n",
      "     |  ProteinBertAbstractModel(config, *inputs, **kwargs)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __init__(self, config, *inputs, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  forward = _forward_unimplemented(self, *input: Any) -> None\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertAttention(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertAttention(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertAttention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_tensor, attention_mask)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  prune_heads(self, heads)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertConfig(tape.models.modeling_utils.ProteinConfig)\n",
      "     |  ProteinBertConfig(vocab_size: int = 30, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 3072, hidden_act: str = 'gelu', hidden_dropout_prob: float = 0.1, attention_probs_dropout_prob: float = 0.1, max_position_embeddings: int = 8096, type_vocab_size: int = 2, initializer_range: float = 0.02, layer_norm_eps: float = 1e-12, **kwargs)\n",
      "     |  \n",
      "     |  :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |  configuration of a `ProteinBertModel`.\n",
      "     |  \n",
      "     |  \n",
      "     |  Arguments:\n",
      "     |      vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |          `ProteinBertModel`.\n",
      "     |      hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |      num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |      num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |          the ProteinBert encoder.\n",
      "     |      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |          layer in the ProteinBert encoder.\n",
      "     |      hidden_act: The non-linear activation function (function or string) in the\n",
      "     |          encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |      hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |          layers in the embeddings, encoder, and pooler.\n",
      "     |      attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |          probabilities.\n",
      "     |      max_position_embeddings: The maximum sequence length that this model might\n",
      "     |          ever be used with. Typically set this to something large just in case\n",
      "     |          (e.g., 512 or 1024 or 2048).\n",
      "     |      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |          `ProteinBertModel`.\n",
      "     |      initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |          initializing all weight matrices.\n",
      "     |      layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertConfig\n",
      "     |      tape.models.modeling_utils.ProteinConfig\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size: int = 30, hidden_size: int = 768, num_hidden_layers: int = 12, num_attention_heads: int = 12, intermediate_size: int = 3072, hidden_act: str = 'gelu', hidden_dropout_prob: float = 0.1, attention_probs_dropout_prob: float = 0.1, max_position_embeddings: int = 8096, type_vocab_size: int = 2, initializer_range: float = 0.02, layer_norm_eps: float = 1e-12, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  pretrained_config_archive_map = {'bert-base': 'https://s3.amazonaws.co...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinConfig:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a configuration object to the directory `save_directory`, so that it\n",
      "     |      can be re-loaded using the :func:`~ProteinConfig.from_pretrained`\n",
      "     |      class method.\n",
      "     |  \n",
      "     |  to_dict(self)\n",
      "     |      Serializes this instance to a Python dictionary.\n",
      "     |  \n",
      "     |  to_json_file(self, json_file_path)\n",
      "     |      Save this instance to a json file.\n",
      "     |  \n",
      "     |  to_json_string(self)\n",
      "     |      Serializes this instance to a JSON string.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinConfig:\n",
      "     |  \n",
      "     |  from_dict(json_object) from builtins.type\n",
      "     |      Constructs a `Config` from a Python dictionary of parameters.\n",
      "     |  \n",
      "     |  from_json_file(json_file) from builtins.type\n",
      "     |      Constructs a `BertConfig` from a json file of parameters.\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, **kwargs) from builtins.type\n",
      "     |      Instantiate a :class:`~ProteinConfig`\n",
      "     |           (or a derived class) from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model configuration to\n",
      "     |                load from cache or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing a configuration file saved using the\n",
      "     |                :func:`~ProteinConfig.save_pretrained` method,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |              - a path or url to a saved configuration JSON `file`,\n",
      "     |                e.g.: ``./my_model_directory/configuration.json``.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) dict:\n",
      "     |              key/value pairs with which to update the configuration object after loading.\n",
      "     |      \n",
      "     |              - The values in kwargs of any keys which are configuration attributes will\n",
      "     |                be used to override the loaded values.\n",
      "     |              - Behavior concerning key/value pairs whose keys are *not* configuration\n",
      "     |                attributes is controlled by the `return_unused_kwargs` keyword parameter.\n",
      "     |      \n",
      "     |          return_unused_kwargs: (`optional`) bool:\n",
      "     |      \n",
      "     |              - If False, then this function returns just the final configuration object.\n",
      "     |              - If True, then this functions returns a tuple `(config, unused_kwargs)`\n",
      "     |                where `unused_kwargs` is a dictionary consisting of the key/value pairs\n",
      "     |                whose keys are not configuration attributes: ie the part of kwargs which\n",
      "     |                has not been used to update `config` and is otherwise ignored.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # We can't instantiate directly the base class `ProteinConfig` so let's\n",
      "     |            show the examples on a derived class: ProteinBertConfig\n",
      "     |          # Download configuration from S3 and cache.\n",
      "     |          config = ProteinBertConfig.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          config = ProteinBertConfig.from_pretrained('./test/saved_model/')\n",
      "     |          config = ProteinBertConfig.from_pretrained(\n",
      "     |              './test/saved_model/my_configuration.json')\n",
      "     |          config = ProteinBertConfig.from_pretrained(\n",
      "     |              'bert-base-uncased', output_attention=True, foo=False)\n",
      "     |          assert config.output_attention == True\n",
      "     |          config, unused_kwargs = BertConfig.from_pretrained(\n",
      "     |              'bert-base-uncased', output_attention=True,\n",
      "     |              foo=False, return_unused_kwargs=True)\n",
      "     |          assert config.output_attention == True\n",
      "     |          assert unused_kwargs == {'foo': False}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from tape.models.modeling_utils.ProteinConfig:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinConfig:\n",
      "     |  \n",
      "     |  __annotations__ = {'pretrained_config_archive_map': typing.Dict[str, s...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class ProteinBertEmbeddings(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertEmbeddings(config)\n",
      "     |  \n",
      "     |  Construct the embeddings from word, position and token_type embeddings.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertEmbeddings\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, token_type_ids=None, position_ids=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertEncoder(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertEncoder(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertEncoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states, attention_mask, chunks=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  run_function(self, start, chunk_size)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertForContactPrediction(ProteinBertAbstractModel)\n",
      "     |  ProteinBertForContactPrediction(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertForContactPrediction\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, protein_length, input_mask=None, targets=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertForMaskedLM(ProteinBertAbstractModel)\n",
      "     |  ProteinBertForMaskedLM(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertForMaskedLM\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, input_mask=None, targets=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  tie_weights(self)\n",
      "     |      Make sure we are sharing the input and output embeddings.\n",
      "     |      Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertForSequenceClassification(ProteinBertAbstractModel)\n",
      "     |  ProteinBertForSequenceClassification(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertForSequenceClassification\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, input_mask=None, targets=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertForSequenceToSequenceClassification(ProteinBertAbstractModel)\n",
      "     |  ProteinBertForSequenceToSequenceClassification(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertForSequenceToSequenceClassification\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, input_mask=None, targets=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertForValuePrediction(ProteinBertAbstractModel)\n",
      "     |  ProteinBertForValuePrediction(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertForValuePrediction\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, input_mask=None, targets=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertIntermediate(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertIntermediate(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertIntermediate\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertLayer(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertLayer(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertLayer\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states, attention_mask)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertModel(ProteinBertAbstractModel)\n",
      "     |  ProteinBertModel(config)\n",
      "     |  \n",
      "     |  An abstract class to handle weights initialization and\n",
      "     |  a simple interface for dowloading and loading pretrained models.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertModel\n",
      "     |      ProteinBertAbstractModel\n",
      "     |      tape.models.modeling_utils.ProteinModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, input_ids, input_mask=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ProteinBertAbstractModel:\n",
      "     |  \n",
      "     |  base_model_prefix = 'bert'\n",
      "     |  \n",
      "     |  config_class = <class 'tape.models.modeling_bert.ProteinBertConfig'>\n",
      "     |      :class:`~pytorch_transformers.ProteinBertConfig` is the configuration class to store the\n",
      "     |      configuration of a `ProteinBertModel`.\n",
      "     |      \n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in\n",
      "     |              `ProteinBertModel`.\n",
      "     |          hidden_size: Size of the encoder layers and the pooler layer.\n",
      "     |          num_hidden_layers: Number of hidden layers in the ProteinBert encoder.\n",
      "     |          num_attention_heads: Number of attention heads for each attention layer in\n",
      "     |              the ProteinBert encoder.\n",
      "     |          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
      "     |              layer in the ProteinBert encoder.\n",
      "     |          hidden_act: The non-linear activation function (function or string) in the\n",
      "     |              encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
      "     |          hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
      "     |              layers in the embeddings, encoder, and pooler.\n",
      "     |          attention_probs_dropout_prob: The dropout ratio for the attention\n",
      "     |              probabilities.\n",
      "     |          max_position_embeddings: The maximum sequence length that this model might\n",
      "     |              ever be used with. Typically set this to something large just in case\n",
      "     |              (e.g., 512 or 1024 or 2048).\n",
      "     |          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
      "     |              `ProteinBertModel`.\n",
      "     |          initializer_range: The sttdev of the truncated_normal_initializer for\n",
      "     |              initializing all weight matrices.\n",
      "     |          layer_norm_eps: The epsilon used by LayerNorm.\n",
      "     |  \n",
      "     |  pretrained_model_archive_map = {'bert-base': 'https://s3.amazonaws.com...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  init_weights(self)\n",
      "     |      Initialize and prunes weights if needed.\n",
      "     |  \n",
      "     |  prune_heads(self, heads_to_prune)\n",
      "     |      Prunes heads of the base model.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          heads_to_prune: dict with keys being selected layer indices (`int`) and\n",
      "     |              associated values being the list of heads to prune in said layer\n",
      "     |              (list of `int`).\n",
      "     |  \n",
      "     |  resize_token_embeddings(self, new_num_tokens=None)\n",
      "     |      Resize input token embeddings matrix of the model if\n",
      "     |          new_num_tokens != config.vocab_size. Take care of tying weights embeddings\n",
      "     |          afterwards if the model class has a `tie_weights()` method.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |      \n",
      "     |          new_num_tokens: (`optional`) int:\n",
      "     |              New number of tokens in the embedding matrix. Increasing the size will add\n",
      "     |              newly initialized vectors at the end. Reducing the size will remove vectors\n",
      "     |              from the end. If not provided or None: does nothing and just returns a\n",
      "     |              pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n",
      "     |      \n",
      "     |      Return: ``torch.nn.Embeddings``\n",
      "     |          Pointer to the input tokens Embeddings Module of the model\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory)\n",
      "     |      Save a model and its configuration file to a directory, so that it\n",
      "     |      can be re-loaded using the `:func:`~ProteinModel.from_pretrained`\n",
      "     |      ` class method.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs) from builtins.type\n",
      "     |      Instantiate a pretrained pytorch model from a pre-trained model configuration.\n",
      "     |      \n",
      "     |      The model is set in evaluation mode by default using ``model.eval()``\n",
      "     |      (Dropout modules are deactivated)\n",
      "     |      To train the model, you should first set it back in training mode with ``model.train()``\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not initialized from pretrained model`` means that\n",
      "     |      the weights of XXX do not come pre-trained with the rest of the model.\n",
      "     |      It is up to you to train those weights with a downstream fine-tuning task.\n",
      "     |      \n",
      "     |      The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used\n",
      "     |      by YYY, therefore those weights are discarded.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          pretrained_model_name_or_path: either:\n",
      "     |      \n",
      "     |              - a string with the `shortcut name` of a pre-trained model to load from cache\n",
      "     |                or download, e.g.: ``bert-base-uncased``.\n",
      "     |              - a path to a `directory` containing model weights saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained`,\n",
      "     |                e.g.: ``./my_model_directory/``.\n",
      "     |      \n",
      "     |          model_args: (`optional`) Sequence of positional arguments:\n",
      "     |              All remaning positional arguments will be passed to the underlying model's\n",
      "     |              ``__init__`` method\n",
      "     |      \n",
      "     |          config: (`optional`) instance of a class derived from\n",
      "     |              :class:`~ProteinConfig`: Configuration for the model to\n",
      "     |              use instead of an automatically loaded configuation. Configuration can be\n",
      "     |              automatically loaded when:\n",
      "     |      \n",
      "     |              - the model is a model provided by the library (loaded with the\n",
      "     |                ``shortcut-name`` string of a pretrained model), or\n",
      "     |              - the model was saved using\n",
      "     |                :func:`~ProteinModel.save_pretrained` and is reloaded\n",
      "     |                by suppling the save directory.\n",
      "     |              - the model is loaded by suppling a local directory as\n",
      "     |                ``pretrained_model_name_or_path`` and a configuration JSON file named\n",
      "     |                `config.json` is found in the directory.\n",
      "     |      \n",
      "     |          state_dict: (`optional`) dict:\n",
      "     |              an optional state dictionnary for the model to use instead of a state\n",
      "     |              dictionary loaded from saved weights file. This option can be used if you\n",
      "     |              want to create a model from a pretrained configuration but load your own\n",
      "     |              weights. In this case though, you should check if using\n",
      "     |              :func:`~ProteinModel.save_pretrained` and\n",
      "     |              :func:`~ProteinModel.from_pretrained` is not a\n",
      "     |              simpler option.\n",
      "     |      \n",
      "     |          cache_dir: (`optional`) string:\n",
      "     |              Path to a directory in which a downloaded pre-trained model\n",
      "     |              configuration should be cached if the standard cache should not be used.\n",
      "     |      \n",
      "     |          force_download: (`optional`) boolean, default False:\n",
      "     |              Force to (re-)download the model weights and configuration files and override\n",
      "     |              the cached versions if they exists.\n",
      "     |      \n",
      "     |          resume_download: (`optional`) boolean, default False:\n",
      "     |              Do not delete incompletely recieved file. Attempt to resume the download if\n",
      "     |              such a file exists.\n",
      "     |      \n",
      "     |          output_loading_info: (`optional`) boolean:\n",
      "     |              Set to ``True`` to also return a dictionnary containing missing keys,\n",
      "     |              unexpected keys and error messages.\n",
      "     |      \n",
      "     |          kwargs: (`optional`) Remaining dictionary of keyword arguments:\n",
      "     |              Can be used to update the configuration object (after it being loaded) and\n",
      "     |              initiate the model. (e.g. ``output_attention=True``). Behave differently\n",
      "     |              depending on whether a `config` is provided or automatically loaded:\n",
      "     |      \n",
      "     |              - If a configuration is provided with ``config``, ``**kwarg\n",
      "     |                directly passed to the underlying model's ``__init__`` method (we assume\n",
      "     |                all relevant updates to the configuration have already been done)\n",
      "     |              - If a configuration is not provided, ``kwargs`` will be first passed to the\n",
      "     |                configuration class initialization function\n",
      "     |                (:func:`~ProteinConfig.from_pretrained`). Each key of\n",
      "     |                ``kwargs`` that corresponds to a configuration attribute will be used to\n",
      "     |                override said attribute with the supplied ``kwargs`` value. Remaining keys\n",
      "     |                that do not correspond to any configuration attribute will be passed to the\n",
      "     |                underlying model's ``__init__`` function.\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          # Download model and configuration from S3 and cache.\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased')\n",
      "     |          # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
      "     |          model = ProteinBertModel.from_pretrained('./test/saved_model/')\n",
      "     |          # Update configuration during loading\n",
      "     |          model = ProteinBertModel.from_pretrained('bert-base-uncased', output_attention=True)\n",
      "     |          assert model.config.output_attention == True\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from tape.models.modeling_utils.ProteinModel:\n",
      "     |  \n",
      "     |  __annotations__ = {'config_class': typing.Type[tape.models.modeling_ut...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertOutput(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertOutput(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertOutput\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states, input_tensor)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertPooler(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertPooler(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertPooler\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertSelfAttention(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertSelfAttention(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertSelfAttention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states, attention_mask)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  transpose_for_scores(self, x)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class ProteinBertSelfOutput(torch.nn.modules.module.Module)\n",
      "     |  ProteinBertSelfOutput(config)\n",
      "     |  \n",
      "     |  Base class for all neural network modules.\n",
      "     |  \n",
      "     |  Your models should also subclass this class.\n",
      "     |  \n",
      "     |  Modules can also contain other Modules, allowing to nest them in\n",
      "     |  a tree structure. You can assign the submodules as regular attributes::\n",
      "     |  \n",
      "     |      import torch.nn as nn\n",
      "     |      import torch.nn.functional as F\n",
      "     |  \n",
      "     |      class Model(nn.Module):\n",
      "     |          def __init__(self):\n",
      "     |              super(Model, self).__init__()\n",
      "     |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "     |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "     |  \n",
      "     |          def forward(self, x):\n",
      "     |              x = F.relu(self.conv1(x))\n",
      "     |              return F.relu(self.conv2(x))\n",
      "     |  \n",
      "     |  Submodules assigned in this way will be registered, and will have their\n",
      "     |  parameters converted too when you call :meth:`to`, etc.\n",
      "     |  \n",
      "     |  :ivar training: Boolean represents whether this module is in training or\n",
      "     |                  evaluation mode.\n",
      "     |  :vartype training: bool\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ProteinBertSelfOutput\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, config)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, hidden_states, input_tensor)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Default dir() implementation.\n",
      "     |  \n",
      "     |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self: ~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self: ~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to float datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  half(self: ~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '')\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor): buffer to be registered.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter): parameter to be added to the module.\n",
      "     |  \n",
      "     |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  share_memory(self: ~T) -> ~T\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  train(self: ~T, mode: bool = True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none: bool = False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "\n",
      "DATA\n",
      "    BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {'bert-base': 'https://s3.amazona...\n",
      "    BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {'bert-base': 'https://s3.amazonaw...\n",
      "    URL_PREFIX = 'https://s3.amazonaws.com/songlabdata/proteindata/pytorch...\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 1310...\n",
      "    logger = <Logger tape.models.modeling_bert (WARNING)>\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    registry = <tape.registry.Registry object>\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "\n",
      "FILE\n",
      "    /home/bli/.conda/envs/Xihe/lib/python3.8/site-packages/tape/models/modeling_bert.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tape.models.modeling_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b25ec7ad268aafa8a32058e7e379c0da99097d3894e9780b56b3ba157de509ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Xihe': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
